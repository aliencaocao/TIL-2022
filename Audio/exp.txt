effecient net v2s no db norm, 9k files (removed bad files), 20 epoch: 7232
effecient net v2s with db norm, 9531 files (after fixed bad files using norm), 20 epoch: 7331
all exp below uses 9531 files
densenet121 with densenet preprocessing layer, 20 epoch: 2071
densenet121 NO densenet preprocessing layer, 20 epoch: 6917
effecient net v2s with no aug, crashed at 11 epoch but reducelronplaetu set to 3 (prev was 4): 7593-> improvement might be caused by callback change
effecient net v2s with no aug, 20 epoch with consine decay (20 * 477 steps), no reducelr (cannot use tger): 7577, overfits hard to 0.9977 on train
effecient net v2s with aug, 20 epoch with consine decay (20 * 477 steps), no reducelr (cannot use tger): 7420, overfits hard to 0.9929 on train
---seems like LR scheduler lead to more overfit on train but val doesnt hurt a lot
effecient net v2s with aug, 20 epoch with reducelr=3, no schedule: 7441, seems no aug improves a bit

movilenetv3 pretrained, no aug, batchsize 32, no schedule: 7556 (overfits hard to 0.9975 on train), initial lr 1e-3 but didnt converage on val until reduced to 1e-4
movilenetv3 pretrained, no aug, batchsize 32, initiallr 5e-4, no schedule: 7520 (overfits hard to 0.9999 on train)
movilenetv3 pretrained, with aug, batchsize 32, initiallr 5e-4, no schedule: 7252 (overfits hard to 0.9999 on train)
movilenetv3 pretrained, with aug, batchsize 32, initiallr 5e-4, no schedule, no TTA: 7499 (overfits hard to 0.9999 on train)

ResNetV2 50 pretrained, with aug: 7499
ResNetV2 50 pretrained, no aug: 7504

RegNetY008 no aug: 7630
RegNetY008 no aug, dense256 reduced to 128, dropout increase to 0.3: 7520
RegNetY008 no aug, lr=1e-3: 7084
RegNetY008 with aug, lr=5e-4: 7604
RegNetY016 no aug, lr=5e-4: 7520
RegNetY016 no aug, lr=1e-3: 7562
RegNetY008 no aug, dense256, dropout 0.3: 7661
RegNetY008 no aug, lr schedule initial 1e-3: 7520
RegNetY008 no aug, lr=5e-4, replaced all relu with swish: 7593
RegNetY004 no aug, lr=5e-4, replaced all relu with swish: 7011
RegNetY016 no aug, dense256, dropout 0.3, lr=1e-3: 7661 (may be btr if longer epoch)
RegNet X 016 no aug, dense256, dropout 0.3, lr=1e-3: 7813 (extended to 22 epoch)
RegNet X 016 no aug, dense256, dropout 0.3, 22 epoch early stopped, lr=5e-4: 7745
RegNet X 008 no aug, dense256, dropout 0.3, 19 epoch early stopped, lr=5e-4: 7598
RegNet X 032 no aug, dense256, dropout 0.3, 14 epoch early stopped, lr=5e-4: 7682
RegNet X 032 no aug, dense256, dropout 0.3, 14 epoch early stopped, lr=5e-4: 7682
RegNet X 032 no aug, dense256, dropout 0.3, 25 epoch, lr=1e-3: 7813 (5e-4 abt 76)
RegNet X 032 no aug, dense256, dropout 0.3, 25 epoch, lr=1e-3, melbin fmin=50, fmax=8000: 7556
RegNet X 016 no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1: best acc 7803 but best F1 is 7735 which correspond to acc 7738
Same as above but with aug: 2*time mask=10, 2*freq mask=10, TTA: ~71 f1, bad
RegNet X 016 no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8529, acc: 8512
RegNet X 016 no aug, 1x dense 256, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 5957, acc: 7900
RegNet X 016 no aug, 2x dense 128, dropout 0.4, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8386, acc: 8543
RegNet X 016 with aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8000, acc: 8029

ALl combined data below means combined + TIL train
RegNet X 016 no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1, combined data: f1: 7905, acc: 7979
RegNet X 008 no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 7956, acc:8343
RegNet X 032 no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8544, acc: 8700
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8854(extended to 31 epoch as still increasing) -> global avg pooling GOOD!
RegNet X 064 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 25 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8755, acc: 8900 (extended to 34 epoch)

all combined below add NCSE dataset
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 28 epoch early stopped, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1, combined: f1:7663

F1 below change from macro avg to weighted avg
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 40 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 8701 (BEST REAL SO FAR)

F1 below use TIL test ans as val, test ans score on leaderboard: 8647
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 40 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 trained on TIL train ONLY: f1: 5162
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 40 epoch, lr=1e-3 with ranger, callbacks using F1 score, label smoothing=0.1 combined: f1: 5295
RegNet X 032 + global avg pool 2d no aug, 2x dense 128, dropout 0.3, 40 epoch, lr=7e-4 with ranger, callbacks using F1 score, label smoothing=0.1 combined: f1: 5730